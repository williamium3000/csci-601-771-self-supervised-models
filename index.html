<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">

    <meta property="og:site_name" content="CSCI 601.771 (Semi-supervised Models)">
    <meta property="og:type" content="article">
    <meta property="og:title" content="CSCI 601.771 (Semi-supervised Models)">
    <meta property="og:description" content="Disussing latest breakthroughs with Transformers in diverse domains">
    <meta property="og:url" content="https://cs25.stanford.edu/">
    <meta property="og:image" content="https://cs25.stanford.edu/images/cs25.jpeg">


    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="CS25: Tranformers United!">
    <meta name="twitter:description" content="Disussing latest breakthroughs with Transformers in diverse domains">
    <meta name="twitter:url" content="https://cs25.stanford.edu/">
    <meta name="twitter:image" content="https://cs25.stanford.edu/images/cs25.jpeg">
    <meta name="twitter:site" content="@DivGarg9">

    <title>CSCI 601.771: Semi-supervised Models </title>

    <!-- bootstrap -->
    <link rel="stylesheet" href="files/bootstrap.min.css">

    <!-- Google fonts -->
    <link href="files/fonts.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" type="text/css" href="files/style.css">
    <link rel="stylesheet" href="files/font-awesome.min.css">

    <!--favicon-->
    <link rel="shortcut icon" href="files/favicon.ico"/>

</head>

<body data-new-gr-c-s-check-loaded="14.1063.0" data-gr-ext-installed="">

<!-- <script src="header.js"></script> -->
<!-- Navbar -->
<nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <a class="navbar-brand brand" href="index.html">CSCI 601.771</a>
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                    data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>

        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li><a href="#course">Course</a></li>
                <li><a href="#schedule">Schedule</a></li>
                <li><a href="#project">Project</a></li>
                <li><a href="https://github.com/JHU-CLSP/csci-601-771-self-supervised-models">Edit this page!</a></li>
            </ul>
        </div>
    </div>
</nav>

<!-- Header -->
<div id="header" style="text-align:center">
    <!--    <img src="files/blank.png" class="logo-left">-->
    <a href="https://www.cs.jhu.edu/">
        <img src="files/jhu_shield.png" class="logo-right">
    </a>
<!--    <a href="https://www.clsp.jhu.edu/">-->
<!--        <img src="files/clsp-logo.png" class="logo-right">-->
<!--    </a>-->
    <h1>CSCI 601.771: Semi-supervised Statistical Models</h1>
    <h3>Johns Hopkins University - Fall 2022</h3>
    <div style="clear:both;"></div>
</div>

<!-- Intro -->
<div class="container sec" id="intro">
    <p>
        The rise of massive self-supervised (pre-trained) models has transformed various data-driven fields such as
        natural language processing, computer vision, robotics, and medical imaging.
        This advanced graduate course aims to provide a holistic view of the issues related to these models: We will
        start with the history of how we got here, and then delve into the latest success stories. We will then focus on
        the implications of these technologies: social harms, security risks, legal issues, and environmental impacts.
        The class ends with reflections on the future implications of this trajectory.
    </p>

    <p>
        <strong>Prerequisites</strong>:
        Students must have extensive experience with deep learning, machine learning, artificial intelligence, and
        natural language processing.
        <!--  Fluency with linear algebra, statistics and one of the Deep Learning libraries (PyTorch, Tensorflow, JAX, etc.) is highly recommended.--->
        Familiarity with linear algebra, statistics and probability are necessary, as well as with the design and
        implementation of learning models (via one of the learing libraries, such as PyTorch, Tensorflow, Keras, JAX).
        Students must be comfortable with reading papers and extracting key concepts and ideas from papers.
    </p>

</div>

<!-- Staff Info -->
<div class="sechighlight">
    <div class="container sec" id="people">
        <div class="col-md-7" style="width: 100%; text-align: center">
            <h3>Instructors</h3>
            <div class="instructor">
                <a target="_blank" rel="noopener noreferrer" href="http://danielkhashabi.com/">
                    <div class="instructorphoto"><img src="files/daniel.jpeg" alt="missing image"></div>
                    <div>Daniel Khashabi</div>
                </a>
            </div>
            <div class="instructor">
                <a target="_blank" rel="noopener noreferrer" href="?">
                    <div class="instructorphoto"><img src="files/blank.png"></div>
                    <div>J. Doe <br>(Teaching Assistant)</div>
                </a>
            </div>
        </div>
    </div>

    <!-- Logistics -->
    <div class="container sec" id="logistics">
        <h2>Logistics</h2>
        <ul>
            <li><b>Classes</b> are on Tuesday/Thursday 1:30 - 2:45 pm EST (Malone 228)</li>
            <li><b>Class Structure</b>: The class will be in-person. Each session will involve the presentation and/or
                discussion of recent important papers on the self-supervised statistical models. The course also
                involves a project.
            </li>
            <li><b>Coursework</b>: Your grade is based on three activities: paper presentation (45%), in-class
                participation in discussions (15%) and the class project (40%).
            </li>
            <li><b>Contact</b>: If you have any questions about the course, contact us at TBD.</li>
        </ul>
        <br>
    </div>
</div>


<div class="container sec" id="course">
    <h2>Content</h2>
    <p> For much of the semester, each class will involve the presentation and discussion of recent important papers on
        pre-trained (self-supervised) statistical models.
        <!--        The bulk of this class will comprise of talks from researchers discussing latest breakthroughs with transformers-->
        <!--        and explaining how they apply them to their fields of research.-->
        The objective of the course is to instill a holistic view of the latest developments in various fields (NLP,
        computer vision, biology. etc.), and help the participants understand their broad implications.
    </p>

    <p>
        Each paper will be presented by a group of students each with an assigned <a
            href="https://colinraffel.com/blog/role-playing-seminar.html">"role"</a>. This role defines the lens
        through which they read the paper and determines what they prepare for the group in-class discussion.
        Here are the roles we will experiment with:
    </p>
    <ul>
        <li><b>Stakeholder ‚úçÔ∏è:</b> Act as if you're the authors of this paper. Describes their motivation, problem
            definition, method and experimental findings of this paper.
        </li>
        <li><b>Scientific Peer Reviewer üë©üèΩ‚Äçüî¨:</b> Act like you're a reviewer of this work. Be critical of the work,
            though not necessarily
            negative. You can follow the guidelines for <a
                    href="https://nips.cc/Conferences/2020/PaperInformation/ReviewerGuidelines"> NeurIPS reviewers</a>
            (under "Review Content"), taking note of the example reviews included therein.
        </li>
        <li><b>Archaeologist üè∫:</b> Determine where this paper sits in the context of previous and subsequent work.
            Find
            and report on one prior paper that substantially influenced the current paper and one newer paper that cites
            this current paper.
        </li>
        <li><b>Visionary üî≠:</b> Propose an imaginary follow-up research project or a new application -- not just based
            on the current but only possible
            due to the existence and success of the current paper.
        </li>
        <!--        <li><b>Researcher üçú:</b> Propose an imaginary follow-up project &#45;&#45; not just based on the current but only possible-->
        <!--            due to the existence and success of the current paper.-->
        <!--        </li>-->
        <!--        <li><b>Practitioner üñ•Ô∏è:</b> Propose a new application for the method in the paper (not already discussed in class),-->
        <!--            and discuss at least one positive and negative impact of this application.-->
        <!--        </li>-->
    </ul>
    <p>
        The presentation of each role can be done individually or in groups of ‚â§3. If done as a group, you and your
        partner
        should decide how to equally divide the work for a given paper presentation session.
    </p>
    <p><b>Who presents what role and when?</b> At the beginning of the semester, students will be divided into two
        halves,
        one half presenting on Tuesdays and the other on Thursdays.
        <!-- we will split the class into two groups: one group present presents on Tuesdays and the other group presents on Thursdays.-->
        In a given class session, the students in the presenting half will each be given a random role (determined the
        week before at the end of the classes).
        Each role group (irrespective of how many students are assigned to it) should aim for 10 mins presentation.
        You're encouraged to have slides for your role, though it is not mandatory.
        <!--        You should present your findings with a formal presentation, i.e., have some slides prepared for the group-->
        <!--        in-class discussion.-->
        <!--        Your assigned role determines what you should include in the slides.-->
        If you do so, I would recommend less than 5-8 slides to make sure stay within our time budget.
    </p>
    <p><b>What slides?</b> To minimize time spent context switching or fighting with screen sharing/projector dongles,
        we will have a shared pool of slides (hosted on Google presentations, will be shared a week before).
        Each role group are encouraged to title their slides with "<span
                style="font-family: 'Trebuchet MS', sans-serif;">[role emoji]: [student name]</span>" (as in "<span
                style="font-family: 'Trebuchet MS', sans-serif;">üè∫: Jane,John</span>")
        so that the slides are quickly identified during the session.
        If you choose to make slides, you're <b>not</b> expected to
        prepare a full-blown presentation -- they're encouraged for visual aid and facilitating the presentation.
    </p>
    <p><b>What about the non-presenting half?</b>
        While only a subset of the class will participate in presenting a paper, the rest of the class is expected to
        come to class ready to participate in the discussions.
    </p>
</div>

<!--[TODO: how to change the roles? consult with the instructor]-->


<div class="container sec" id="schedule" style="margin-top:-20px">
    <br>
    <h2>Schedule</h2>
    <p> The current class schedule is below (subject to change): </p>

    <table class="table">
        <colgroup>
            <col style="width:15%">
            <col style="width:25%">
            <col style="width:70%">

        </colgroup>
        <thead>
        <tr class="active">
            <th>Date</th>
            <th>Topic</th>
            <th>Course Materials</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td>#1 - Tue Aug 30</td>
            <td>Course overview, plan and expectations</td>
            <td>
                <a href="https://docs.google.com/presentation/d/1tw4UBhcEu_ceWx2EL719htMbZ4YccEsfmxfhB5NKS8U/edit?usp=sharing">Slides</a>
            </td>
        </tr>
        <tr>
            <td>#2 - Thu Sept 1</td>
            <td>Preliminaries: Past, Architectures, Pre-training, Capabilities</td>
            <td>
                <a href="https://docs.google.com/presentation/d/110npLpsPLwwjbBIHIy6VDSHcwL-pRRaU0UPvGe4ls6o/edit?usp=sharing">Slides</a><br>
<!--                Main Reading:-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->
                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All You Need</a></li>
                    <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer  </a></li>
                    <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#3 - Tue Sept 6</td>
            <td>Pretraining Language Models</td>
            <td>
                <a href="https://docs.google.com/presentation/d/1_EPdETs7OnpB8s8iI7p5IIuV83WgIPctD2iYJeFYSLU/edit?usp=sharing">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
                    <li><a href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf">Exploring the limits of transfer learning with a unified text-to-text transformer</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#4 - Thu Sept 8</td>
            <td>Pretraining Language Models</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
                    <li><a href="https://arxiv.org/pdf/2205.01068.pdf">OPT: Open Pre-trained Transformer Language Models</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#5 - Tue Sept 13</td>
            <td>Scaling</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2001.08361v1.pdf">Scaling Laws for Neural Language Models</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2010.14701.pdf">Scaling Laws for Autoregressive Generative Modeling</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#6 - Thu Sept 15</td>
            <td>Scaling</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2203.15556.pdf">Training Compute-Optimal Large Language Models</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/abs/2112.11446">Scaling Language Models: Methods, Analysis & Insights from Training Gopher</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#7 - Tue Sept 20</td>
            <td>Social Harms: Toxicity</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="">RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots:  Can Language Models Be Too Big?</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#8 - Thu Sept 22</td>
            <td>Social Harms: Bias</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://papers.nips.cc/paper/2021/file/1531beb762df4029513ebf9295e0d34f-Paper.pdf">Bias Out-of-the-Box: An Empirical Analysis of  Intersectional Occupational Biases in Popular  Generative Language Models</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2010.02428.pdf">UnQovering Stereotypical Biases via Underspecified Questions.</a></li>
                    <li><a href="https://arxiv.org/pdf/2206.09860.pdf">Fewer Errors, but More Stereotypes?  The Effect of Model Size on Gender Bias.</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#9 - Tue Sept 27</td>
            <td>Data</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2104.08758.pdf">Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2101.00027.pdf">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</a></li>
                </ol>
            </td>

        </tr>
        <tr>
            <td>#10 - Thu Sept 29</td>
            <td>Social Harms: Misinformation</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/abs/1905.12616">Defending against neural fake news</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/1908.09203.pdf">Release Strategies and the  Social Impacts of Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2204.05533.pdf">How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image</a></li>
                    <li><a href="https://arxiv.org/pdf/1911.00650.pdf">Automatic Detection of Generated Text is  Easiest when Humans are Fooled</a></li>
                </ol>
            </td>
        </tr>
        <tr class="sechighlight centered">
            <td>Fri Sept 30 </td>
            <td colspan="2">Project proposal submission deadline </td>
        </tr>
        <tr>
            <td>#11 - Tue Oct 4</td>
            <td>Memorization</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2202.07646.pdf">Quantifying Memorization  Across Neural Language Models</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://www.usenix.org/system/files/sec21-carlini-extracting.pdf">Extracting Training Data from Large Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2112.12938.pdf">Counterfactual Memorization in Neural Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2203.08242.pdf">Data Contamination: From Memorization to Exploitation</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#12 - Thu Oct 6</td>
            <td>Privacy</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/abs/2110.05679">Large Language Models Can Be Strong Differentially Private Learners</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2110.06500.pdf">Differentially Private Fine-tuning of Language Models</a></li>
                    <li><a href="https://pair.withgoogle.com/explorables/private-and-fair/">Can a Model Be Differentially Private and Fair?</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#13 - Tue Oct 11</td>
            <td>External Speaker (TBD)</td>
            <td>
<!--                <a href="http://tbd">Slides</a><br>-->
<!--                Main Reading:-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->

<!--                Additional Reading(s):-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->
            </td>
        </tr>
        <tr class="sechighlight centered">
            <td>#14 - Thu Oct 13</td>
            <td>Project Proposal Presentation</td>
            <td>
                <a href="http://tbd">Slides</a><br>
            </td>
        </tr>

        <tr>
            <td>#15 - Tue Oct 18</td>
                        <td>Pretraining Vision-Language Models</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2103.00020.pdf">Learning Transferable Visual Models From Natural Language Supervision</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2106.02636.pdf">MERLOT: Multimodal Neural Script Knowledge Models</a></li>
<!--                    <li><a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf">Flamingo: a Visual Language Model  for Few-Shot Learning</a></li>-->

                </ol>
            </td>

        </tr>
        <tr>
            <td>#16 - Thu Oct 20</td>

            <td>Pretraining Vision-Language Models</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2102.12092.pdf">Zero-Shot Text-to-Image Generation</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2204.06125.pdf">Hierarchical Text-Conditional Image Generation with CLIP Latents</a></li>
                    <li><a href="https://imagen.research.google/paper.pdf">Photorealistic Text-to-Image Diffusion Models  with Deep Language Understanding</a></li>
<!--                    <li><a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf">Flamingo: a Visual Language Model  for Few-Shot Learning</a></li>-->
                </ol>
            </td>


        </tr>
        <tr>
            <td>#17 - Tue Oct 25</td>
            <td>Pretraining Speech Models</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2006.11477.pdf">wav2vec 2.0: A Framework for Self-Supervised  Learning of Speech Representations</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#18 - Thu Oct 27</td>
            <td>Pretraining Coding Models</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://arxiv.org/pdf/2203.07814.pdf">Competition-Level Code Generation with AlphaCode</a></li>
                    <li><a href="https://arxiv.org/pdf/2204.05999.pdf">InCoder: A Generative Model for Code Infilling and Synthesis</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#19 - Tue Nov 1</td>
            <td>Continuous Prompts</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#20 - Thu Nov 3</td>
            <td>Interpretibility</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#21 - Tue Nov 8</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#22 - Thu Nov 10</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#23 - Tue Nov 15</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#24 - Thu Nov 17</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#25 - Tue Nov 22</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#26 - Thu Nov 24</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#27 - Tue Nov 29</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#28 - Thu Dec 1</td>
            <td>Introduction to Transformers</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href=""></a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href=""></a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#29 - Tue Dec 6</td>
            <td>Environmental Impact</td>
            <td>
                <a href="http://tbd">Slides</a><br>
                Main Reading:
                <ol>
                    <li><a href="https://arxiv.org/pdf/1906.02243.pdf">Energy and Policy Considerations for Deep Learning in NLP</a></li>
                </ol>

                Additional Reading(s):
                <ol>
                    <li><a href="https://dl.acm.org/doi/pdf/10.1145/3381831">Green AI</a></li>
                </ol>
            </td>
        </tr>
        <tr>
            <td>#30 - Thu Dec 8</td>
            <td>External Speaker (TBD)</td>
            <td>
<!--                <a href="http://tbd">Slides</a><br>-->
<!--                Main Reading:-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->

<!--                Additional Reading(s):-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->
            </td>
        </tr>
        <tr class="sechighlight centered">
            <td>Fri Dec 9 </td>
            <td colspan="2">Final report submission deadline </td>
        </tr>
        <tr class="sechighlight">
            <td>Dec 13, 15</td>
            <td>Final Project Presentation</td>
            <td>
                <a href="http://tbd">Slides</a><br>
<!--                Main Reading:-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->

<!--                Additional Reading(s):-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->
            </td>
        </tr>
<!--        <tr>-->
<!--            <td>#32 - Thu Dec 15</td>-->
<!--            <td>Introduction to Transformers</td>-->
<!--            <td>-->
<!--                <a href="http://tbd">Slides</a><br>-->
<!--                Main Reading:-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->

<!--                Additional Reading(s):-->
<!--                <ol>-->
<!--                    <li><a href=""></a></li>-->
<!--                </ol>-->
<!--            </td>-->
<!--        </tr>-->

        </tbody>
    </table>
</div>

<div class="container sechighlight" id="project">
    <h2>Project</h2>
    <p>
        All students in the class will write a "mini-paper" as a final project. The topic of this project is open-ended.
        This project, for example, can focus on demonstrate systemic limitations of a prior work
        or suggesting improvements on methods or benchmarks discussed in the class.
        <!--        reproducing one or more papers covered in the class (or relevant works)-->
        <!--        or extending them.-->
    </p>
    <ul>
        <li><b>Group work:</b> Students are expected to work in groups on the final project (team sizes limited to 2 or
            3 people). Students in groups are required to include a ‚Äúcontributions‚Äù section concretely lists each
            author‚Äôs contributions (see Section 8 of this paper, for example). The length of the final report should be
            5 to 8 pages. Note that longer reports are not necessarily better.
        </li>
        <li><b>Project proposals:</b> All groups will be required to submit a project proposal by the start of class on
            TBD. The project proposal is a single-paragraph description of what you intend to do (experiments, datasets,
            methods, etc.) The instructors will provide feedback on these ideas to help the teams with finding a
            concrete idea. There will be lightning presentations of the finalized proposals on TBD.
        </li>
        <li><b>Midway progress presentation:</b> Groups will present their halfway progress during a mid-way project
            presentation. This is expected to be a short presentation discussing the progress made for each project and
            any remaining hurdles.
        </li>
        <li><b>Final presentation:</b> The final project presentation will be during the final exam period. All students
            in each group are required to present some material during the final presentation.
        </li>
        <li><b>Final Report:</b> Students should write code and carry out additional experiments and then write up the
            results in a standard conference paper format (<a
                    href="https://www.overleaf.com/latex/templates/neurips-2022/kxymzbjpwsqx">template</a>). The final
            reports are due TBD.
        </li>
    </ul>
</div>

<div class="container sec" id="resources">
    <h2>Relevant Resources</h2>
    <p>Here are several resources available for free:</p>
    <ul>
        <li>GPU Access:
            <ul>
                <li>Grad students should have access to the graduate grid, which has GPUs.</li>
                <li>Undergraduate students should have access to the undergrad grid (TODO: check).</li>
                <li><a href="https://colab.research.google.com/">Google Colab</a> provides free GPU usage for up to 12
                    hours/day for academic purposes. One can obtain <a
                            href="https://medium.com/@yufengg/how-to-upgrade-colab-with-more-compute-64d53a9b05dc"> more
                        compute on Colab</a> with relatively minimal pay.
                </li>
                <li><a href="https://www.kaggle.com/general/108481">Kaggle</a> offers GPUs for its users.</li>
                <li><a href="https://aws.amazon.com/education/awseducate/">AWS</a> and <a
                        href="https://azure.microsoft.com/en-us/free/students/">Azure</a> both offer welcome credits to
                    students.
                </li>
            </ul>
        </li>
        <li>Demos:
            <ul>
                <li><a href="https://6b.eleuther.ai">GPT-J demo</a></li>
                <li><a href="https://opt.alpa.ai/#generation">OPT demo</a></li>
                <li><a href="https://huggingface.co/bigscience/bloom">BLOOM demo</a></li>
                <li><a href="https://c4-search.apps.allenai.org">A queryable interface to C4</a></li>
                <li><a href="https://vision-explorer.allenai.org">AllenAI vision demo</a></li>
                <li><a href="https://demo.allennlp.org">AllenNLP demo</a></li>
                <li><a href="https://unqover.apps.allenai.org/">Social stereotypes in models</a></li>
            </ul>
        </li>
        <li>Tutorials:</li>
        <ul>
            <li><a href="https://pytorch.org/tutorials/">These tutorials</a> do a good job of introducing PyTorch.</li>
            <li>A <a href="https://huggingface.co/course/chapter1/1">course</a> on Huggingface's Transformers library.
            </li>
        </ul>
    </ul>
    <p>
        Besides these resources, we will try our best to satisfy individual needs through discussion.
    </p>
</div>


<div class="container sec" id="conduct">
    <h2>Conduct</h2>
    <p>
        Since this is a discussion class, it's especially important that we respect everyone's perspective and input. In
        particular, I value the perspectives of individuals from all backgrounds reflecting the diversity of our
        students. I will strive to make this classroom an inclusive space for all students. Please let me know if there
        is anything I can do to improve.
    </p>
    <p>
        This course will have a zero-tolerance philosophy regarding <a
            href="https://www.cs.jhu.edu/academic-programs/academic-integrity-code/">plagiarism or other forms of
        cheating</a>, and incidents
        of academic dishonesty will be reported. A student who has doubts about how the Honor Code applies to this
        course should obtain specific guidance from the course instructor before submitting the respective assignment.
    </p>
</div>

<div class="container sec" id="relevant-courses">
    <h2>Relevant Courses</h2>
    <ul>
        <li><a href="https://stanford-cs324.github.io/winter2022/"> Stanford CS324 - Large Language Models</a>: content inspiration</li>
        <li><a href="https://web.stanford.edu/class/cs25/"> Stanford CS25: Transformers United!</a>: content inspiration </li>
        <li><a href="https://github.com/craffel/dl3d-seminar"> UNC COMP790: (Deep) Learning with Limited Labeled Data
            (DL3D)</a>: role-playing format inspiration</li>
        <li><a href="https://isminoula.github.io/cs6604SP21/"> Virginia Tech CS 6604: Data Challenges in Machine
            Learning</a></li>
    </ul>
</div>


<!-- jQuery and Bootstrap -->
<script src="files/jquery.min.js"></script>
<script src="files/bootstrap.min.js"></script>


</body>
</html>